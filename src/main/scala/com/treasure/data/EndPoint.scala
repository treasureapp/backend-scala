package com.treasure.data

import java.io.{File, FileWriter}

import akka.actor.{ActorSystem, Props}
import com.typesafe.config.ConfigFactory
import com.typesafe.scalalogging.LazyLogging

/**
  * Created by gcrowell on 2017-06-23.
  */

private object ActorEndPoint {
  def actorSystem = ActorSystem("actor_system")
}

object DownloaderEndPoint extends LazyLogging {
  private val masterRef = ActorEndPoint.actorSystem.actorOf(Props[DownloaderRootActor], name = "master_consumer")

  def handleDataDownloadRequest(dataDownloadRequest: DataDownloadRequest): Unit = {
    masterRef ! dataDownloadRequest
  }
}

object SaveToCsv {
  val config = ConfigFactory.load()

  val dataRootPath = config.getString("treasure.data.root")
  val priceRootPath = config.getString("treasure.data.price")
  val statementRootPath = config.getString("treasure.data.statement")

  def save(request: DataDownloadRequest, data: Seq[Record]): Unit = {
    val destRoot = new File(priceRootPath)
    destRoot.mkdirs()
    val destPath = destRoot + "/" + request.subject.name + ".csv"
    val destFile = new File(destPath)
    if (destFile.isFile) destFile.delete()
    val outFile = new FileWriter(destPath)
    data.foreach { (r: Record) =>
      outFile.write(r.toString + "\n")
    }
    outFile.close()
  }
}

object Spark {

  import org.apache.spark.sql.SparkSession

  val config = ConfigFactory.load()

  val dataRootPath = config.getString("treasure.data.root")
  val priceRootPath = config.getString("treasure.data.price")
  val statementRootPath = config.getString("treasure.data.statement")

  //  val dataRoot = new File(dataRootPath)
  //  val priceRoot = new File(priceRootPath)
  //  val statementRoot = new File(statementRootPath)


  val spark: SparkSession = SparkSession.builder
    .appName("My Spark Application") // optional and will be autogenerated if not specified
    .master("local[*]") // avoid hardcoding the deployment environment
    //    .enableHiveSupport() // self-explanatory, isn't it?
    .config("spark.sql.warehouse.dir", "target/spark-warehouse")
    .getOrCreate

  def getSaveFolder(typeName: String): String = {
    typeName match {
      case "PriceRecord" => {
        priceRootPath
      }
      case "StatementRecord" => {
        statementRootPath
      }
    }
  }

  def save(data: Seq[Record]): Unit = {
    import spark.implicits._

    data match {
      case x: Seq[PriceRecord] => {
        val ds = x.toDS()

        ds.write.save(s"PriceRecord")
      }
      case _ =>
    }
    data.toDS()
  }

  def read: Unit = {
    import spark.implicits._
    val stock_ds_parquet = spark.read.parquet(s"${PriceRecord.toString}").toDF.as[PriceRecord]
    println(s"${stock_ds_parquet.count()} records loaded")
    stock_ds_parquet.show(10)
    println(s"${stock_ds_parquet.count()} records loaded")
  }
}

object EndPoint extends App {

  override def main(args: Array[String]): Unit = {
    val config = ConfigFactory.load()
    println(config.isResolved)
    println(config.getString("treasure.data.root"))
    Spark.read
  }


}

