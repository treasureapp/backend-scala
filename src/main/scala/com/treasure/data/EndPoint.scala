package com.treasure.data

import akka.actor.{ActorSystem, Props}
import com.typesafe.config.ConfigFactory
import com.typesafe.scalalogging.LazyLogging

/**
  * Created by gcrowell on 2017-06-23.
  */

object ActorEndPoint {
  def actorSystem = ActorSystem("actor_system")
}

object DownloaderEndPoint extends LazyLogging {
  private val masterRef = ActorEndPoint.actorSystem.actorOf(Props[DownloaderRootActor], name = "master_consumer")

  def handleDataDownloadRequest(dataDownloadRequest: DataDownloadRequest): Unit = {

  }
}

object Spark {

  import org.apache.spark.sql.SparkSession

  val config = ConfigFactory.load()

  val dataRoot = config.getString("treasure.data.root")

  val spark: SparkSession = SparkSession.builder
    .appName("My Spark Application") // optional and will be autogenerated if not specified
    .master("local[*]") // avoid hardcoding the deployment environment
    //      .enableHiveSupport() // self-explanatory, isn't it?
    .config("spark.sql.warehouse.dir", "target/spark-warehouse")
    .getOrCreate

  def save[A <: Record](data: Seq[Record]): Unit = {
    import spark.implicits._

    data match {
      case x: Seq[PriceRecord] => x.toDS().write.save("foo")
      case _ =>
    }
    data.toDS()

  }

  def read: Unit = {
    import spark.implicits._
    val stock_ds_parquet = spark.read.parquet("foo").toDF.as[PriceRecord]
    println(s"${stock_ds_parquet.count()} records loaded")
    stock_ds_parquet.show(10)
    println(s"${stock_ds_parquet.count()} records loaded")

  }



}

object EndPoint extends App {

  override def main(args: Array[String]): Unit = {
    val config = ConfigFactory.load()
    println(config.isResolved)
    println(config.getString("treasure.data.root"))
    Spark.read
  }




}

